{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Initial Checks"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":962,"status":"ok","timestamp":1670745237736,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"pPrHm-brWgaQ","outputId":"d918d9e3-0fff-4c96-89ab-9f1e32bc43f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed May  8 19:29:02 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 546.33                 Driver Version: 546.33       CUDA Version: 12.3     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  | 00000000:0B:00.0  On |                  N/A |\n","|  0%   49C    P8              27W / 220W |   1114MiB /  8192MiB |     12%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|    0   N/A  N/A      2040    C+G   ...e Stream\\90.0.3.0\\GoogleDriveFS.exe    N/A      |\n","|    0   N/A  N/A      5220    C+G   D:\\PROG\\Microsoft VS Code\\Code.exe        N/A      |\n","|    0   N/A  N/A      5820    C+G   ...b3d8bbwe\\Microsoft.Media.Player.exe    N/A      |\n","|    0   N/A  N/A     10220    C+G   ...oys\\PowerToys.PdfPreviewHandler.exe    N/A      |\n","|    0   N/A  N/A     10360    C+G   C:\\Windows\\explorer.exe                   N/A      |\n","|    0   N/A  N/A     12004    C+G   ...ys\\WinUI3Apps\\PowerToys.Peek.UI.exe    N/A      |\n","|    0   N/A  N/A     12208    C+G   ...\\PowerToys\\PowerToys.FancyZones.exe    N/A      |\n","|    0   N/A  N/A     12312    C+G   ...werToys\\PowerToys.PowerLauncher.exe    N/A      |\n","|    0   N/A  N/A     14648    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n","|    0   N/A  N/A     15096    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n","|    0   N/A  N/A     15928    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n","|    0   N/A  N/A     15972    C+G   ...al\\Discord\\app-1.0.9044\\Discord.exe    N/A      |\n","|    0   N/A  N/A     17428    C+G   ...on\\123.0.2420.97\\msedgewebview2.exe    N/A      |\n","|    0   N/A  N/A     18232    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe    N/A      |\n","|    0   N/A  N/A     19240    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n","|    0   N/A  N/A     21660    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n","|    0   N/A  N/A     22484    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n","|    0   N/A  N/A     22676    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n","|    0   N/A  N/A     23688    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n","|    0   N/A  N/A     30152    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe    N/A      |\n","|    0   N/A  N/A     31272    C+G   ...Data\\Local\\Programs\\Opera\\opera.exe    N/A      |\n","|    0   N/A  N/A     36168    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n","|    0   N/A  N/A     36660    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n","|    0   N/A  N/A     42668    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n","|    0   N/A  N/A     44680    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" Volume in drive D is Depolama1\n"," Volume Serial Number is 166C-5C6C\n","\n"," Directory of d:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\n","\n","08.05.2024  19:19    <DIR>          .\n","08.05.2024  19:19    <DIR>          ..\n","28.04.2024  23:13             3.352 .gitignore\n","28.04.2024  22:39    <DIR>          datasets\n","16.04.2024  23:53                32 README.md\n","08.05.2024  19:27            12.566 train_pipeline.ipynb\n","28.04.2024  23:30               803 troubleshoot_notes.txt\n","28.04.2024  23:27               667 usage_examples_notes.txt\n","28.04.2024  22:52    <DIR>          yolov9\n","               5 File(s)         17.420 bytes\n","               4 Dir(s)  166.920.437.760 bytes free\n"]}],"source":["!dir"]},{"cell_type":"markdown","metadata":{},"source":["### Configuration"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["#-- Helper Functions\n","def now():\n","    from datetime import datetime\n","    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\n","yaml configurations set:\n","DATASET_NAME   = 'set1'\n","BASE_MODEL     = 'yolov9-c'\n","PRETRAINED     = 'yolov9-c.pt'\n","MODEL_CFG_FILE = 'yolov9-c_uav'\n","DATA_CFG_FILE  = 'uav_set1'\n","HYP_FILE       = 'hyp.scratch-high'\n"]}],"source":["%cd \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\"\n","\n","#-- Config\n","DATASET_NAME    = \"set1\" # \"dummy_dataset1\" # \"set2\"\n","BASE_MODEL      = \"yolov9-c\"\n","\n","PRETRAINED      = f\"{BASE_MODEL}.pt\" \n","MODEL_CFG_FILE  = f\"{BASE_MODEL}_uav\"\n","DATA_CFG_FILE   = f\"uav_{DATASET_NAME}\"\n","HYP_FILE        = f\"hyp.scratch-high\"\n","\n","import yaml\n","config = {\n","    \"train\"   : f\"../datasets/{DATASET_NAME}/train/images/\",\n","    \"val\"     : f\"../datasets/{DATASET_NAME}/valid/images/\",\n","    \"test\"    : f\"../datasets/{DATASET_NAME}/test/images/\",\n","    \"nc\"      : 1,\n","    # \"names\"   : {0: 'UAV'}\n","    \"names\"   : ['UAV']\n","}\n"," \n","with open(f\"yolov9/data/{DATA_CFG_FILE}.yaml\", \"w\") as file:\n","    yaml.dump(config, file, default_flow_style=False)\n","\n","print(\"yaml configurations set:\")\n","print(f\"{DATASET_NAME   = }\")\n","print(f\"{BASE_MODEL     = }\")\n","print(f\"{PRETRAINED     = }\")\n","print(f\"{MODEL_CFG_FILE = }\")\n","print(f\"{DATA_CFG_FILE  = }\")\n","print(f\"{HYP_FILE       = }\")"]},{"cell_type":"markdown","metadata":{},"source":["### HYPER-PARAMETERS"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1670745293139,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"fjU5Ph9L8gkh","outputId":"bf5897e7-d1d0-4cdd-d930-2389a08d493c"},"outputs":[{"name":"stdout","output_type":"stream","text":["yolov9-c_uav_size480_ep20_batch16_dataset-set1_optim-Adam_2024-05-08_20-54-20\n"]}],"source":["#-- set hyperparameters \n","IMG_SIZE        = 480 # 360 # 640\n","BATCH_SIZE      = 16 # 8\n","EPOCHS          = 20\n","WORKERS         = 16\n","FREEZE          = 10 \n","PATIENCE        = 5 # early stop after PATIENCE epochs\n","OPTIMIZER       = \"Adam\" # ['SGD', 'Adam', 'AdamW', 'LION']\n","\n","RUN_NAME = f\"{MODEL_CFG_FILE}_size{IMG_SIZE}_ep{EPOCHS}_batch{BATCH_SIZE}_dataset-{DATASET_NAME}_optim-{OPTIMIZER}_{now()}\"\n","\n","print(RUN_NAME)"]},{"cell_type":"markdown","metadata":{},"source":["### Train"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":270,"status":"ok","timestamp":1670745441913,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"Zi86Kf-WKOyv"},"outputs":[{"name":"stdout","output_type":"stream","text":["empty_cache\n"]}],"source":["import torch\n","torch.cuda.empty_cache()\n","print(\"empty_cache\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3325522,"status":"ok","timestamp":1670748768579,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"fzBLKkQixMVK","outputId":"7366e109-0166-4c53-b476-9631ff950743"},"outputs":[{"name":"stdout","output_type":"stream","text":["[WinError 2] Sistem belirtilen dosyayı bulamıyor: 'yolov9'\n","D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\n","W&B disabled.\n"]},{"name":"stdout","output_type":"stream","text":["[WinError 2] Sistem belirtilen dosyayı bulamıyor: 'yolov9'\n","D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\n","W&B disabled.\n"]},{"name":"stdout","output_type":"stream","text":["init\n","[WinError 2] Sistem belirtilen dosyayı bulamıyor: 'yolov9'\n","D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\n","W&B disabled.\n","begin train\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\train_dual.py\", line 12, in <module>\n","    import torch\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n","    raise err\n","OSError: [WinError 1455] Bu işlemin tamamlanması için disk belleği dosyası çok küçük. Error loading \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n","\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=weights/yolov9-c.pt, cfg=models/detect/yolov9-c_uav.yaml, data=data/uav_set1.yaml, hyp=data/hyps/hyp.scratch-high.yaml, epochs=20, batch_size=16, imgsz=480, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=Adam, sync_bn=False, workers=16, project=runs\\train, name=yolov9-c_uav_size480_ep20_batch16_dataset-set1_optim-Adam_2024-05-08_20-54-20, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=5, freeze=[10], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","YOLO  2024-4-14 Python-3.10.8 torch-1.13.1 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO  in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO  runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1         0  models.common.Silence                   []                            \n","  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n","  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n","  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n","  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n","  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n","  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n","  8                -1  1    656384  models.common.ADown                     [512, 512]                    \n","  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n"," 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 7]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n"," 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 15           [-1, 5]  1         0  models.common.Concat                    [1]                           \n"," 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n"," 17                -1  1    164352  models.common.ADown                     [256, 256]                    \n"," 18          [-1, 13]  1         0  models.common.Concat                    [1]                           \n"," 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n"," 20                -1  1    656384  models.common.ADown                     [512, 512]                    \n"," 21          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n"," 23                 5  1    131328  models.common.CBLinear                  [512, [256]]                  \n"," 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]             \n"," 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]        \n"," 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n"," 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n"," 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n"," 29                -1  1    164352  models.common.ADown                     [256, 256]                    \n"," 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]                   \n"," 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n"," 32                -1  1    656384  models.common.ADown                     [512, 512]                    \n"," 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]                      \n"," 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n"," 35                -1  1    656384  models.common.ADown                     [512, 512]                    \n"," 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]                         \n"," 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n"," 38[31, 34, 37, 16, 19, 22]  1  21542822  models.yolo.DualDDetect                 [1, [512, 512, 512, 256, 512, 512]]\n","yolov9-c_uav summary: 962 layers, 50999590 parameters, 50999558 gradients, 238.9 GFLOPs\n","\n","Transferred 1448/1460 items from weights\\yolov9-c.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n","freezing model.1.conv.weight\n","freezing model.1.bn.weight\n","freezing model.1.bn.bias\n","freezing model.2.conv.weight\n","freezing model.2.bn.weight\n","freezing model.2.bn.bias\n","freezing model.3.cv1.conv.weight\n","freezing model.3.cv1.bn.weight\n","freezing model.3.cv1.bn.bias\n","freezing model.3.cv2.0.cv1.conv.weight\n","freezing model.3.cv2.0.cv1.bn.weight\n","freezing model.3.cv2.0.cv1.bn.bias\n","freezing model.3.cv2.0.cv2.conv.weight\n","freezing model.3.cv2.0.cv2.bn.weight\n","freezing model.3.cv2.0.cv2.bn.bias\n","freezing model.3.cv2.0.cv3.conv.weight\n","freezing model.3.cv2.0.cv3.bn.weight\n","freezing model.3.cv2.0.cv3.bn.bias\n","freezing model.3.cv2.0.m.0.cv1.conv1.conv.weight\n","freezing model.3.cv2.0.m.0.cv1.conv1.bn.weight\n","freezing model.3.cv2.0.m.0.cv1.conv1.bn.bias\n","freezing model.3.cv2.0.m.0.cv1.conv2.conv.weight\n","freezing model.3.cv2.0.m.0.cv1.conv2.bn.weight\n","freezing model.3.cv2.0.m.0.cv1.conv2.bn.bias\n","freezing model.3.cv2.0.m.0.cv2.conv.weight\n","freezing model.3.cv2.0.m.0.cv2.bn.weight\n","freezing model.3.cv2.0.m.0.cv2.bn.bias\n","freezing model.3.cv2.1.conv.weight\n","freezing model.3.cv2.1.bn.weight\n","freezing model.3.cv2.1.bn.bias\n","freezing model.3.cv3.0.cv1.conv.weight\n","freezing model.3.cv3.0.cv1.bn.weight\n","freezing model.3.cv3.0.cv1.bn.bias\n","freezing model.3.cv3.0.cv2.conv.weight\n","freezing model.3.cv3.0.cv2.bn.weight\n","freezing model.3.cv3.0.cv2.bn.bias\n","freezing model.3.cv3.0.cv3.conv.weight\n","freezing model.3.cv3.0.cv3.bn.weight\n","freezing model.3.cv3.0.cv3.bn.bias\n","freezing model.3.cv3.0.m.0.cv1.conv1.conv.weight\n","freezing model.3.cv3.0.m.0.cv1.conv1.bn.weight\n","freezing model.3.cv3.0.m.0.cv1.conv1.bn.bias\n","freezing model.3.cv3.0.m.0.cv1.conv2.conv.weight\n","freezing model.3.cv3.0.m.0.cv1.conv2.bn.weight\n","freezing model.3.cv3.0.m.0.cv1.conv2.bn.bias\n","freezing model.3.cv3.0.m.0.cv2.conv.weight\n","freezing model.3.cv3.0.m.0.cv2.bn.weight\n","freezing model.3.cv3.0.m.0.cv2.bn.bias\n","freezing model.3.cv3.1.conv.weight\n","freezing model.3.cv3.1.bn.weight\n","freezing model.3.cv3.1.bn.bias\n","freezing model.3.cv4.conv.weight\n","freezing model.3.cv4.bn.weight\n","freezing model.3.cv4.bn.bias\n","freezing model.4.cv1.conv.weight\n","freezing model.4.cv1.bn.weight\n","freezing model.4.cv1.bn.bias\n","freezing model.4.cv2.conv.weight\n","freezing model.4.cv2.bn.weight\n","freezing model.4.cv2.bn.bias\n","freezing model.5.cv1.conv.weight\n","freezing model.5.cv1.bn.weight\n","freezing model.5.cv1.bn.bias\n","freezing model.5.cv2.0.cv1.conv.weight\n","freezing model.5.cv2.0.cv1.bn.weight\n","freezing model.5.cv2.0.cv1.bn.bias\n","freezing model.5.cv2.0.cv2.conv.weight\n","freezing model.5.cv2.0.cv2.bn.weight\n","freezing model.5.cv2.0.cv2.bn.bias\n","freezing model.5.cv2.0.cv3.conv.weight\n","freezing model.5.cv2.0.cv3.bn.weight\n","freezing model.5.cv2.0.cv3.bn.bias\n","freezing model.5.cv2.0.m.0.cv1.conv1.conv.weight\n","freezing model.5.cv2.0.m.0.cv1.conv1.bn.weight\n","freezing model.5.cv2.0.m.0.cv1.conv1.bn.bias\n","freezing model.5.cv2.0.m.0.cv1.conv2.conv.weight\n","freezing model.5.cv2.0.m.0.cv1.conv2.bn.weight\n","freezing model.5.cv2.0.m.0.cv1.conv2.bn.bias\n","freezing model.5.cv2.0.m.0.cv2.conv.weight\n","freezing model.5.cv2.0.m.0.cv2.bn.weight\n","freezing model.5.cv2.0.m.0.cv2.bn.bias\n","freezing model.5.cv2.1.conv.weight\n","freezing model.5.cv2.1.bn.weight\n","freezing model.5.cv2.1.bn.bias\n","freezing model.5.cv3.0.cv1.conv.weight\n","freezing model.5.cv3.0.cv1.bn.weight\n","freezing model.5.cv3.0.cv1.bn.bias\n","freezing model.5.cv3.0.cv2.conv.weight\n","freezing model.5.cv3.0.cv2.bn.weight\n","freezing model.5.cv3.0.cv2.bn.bias\n","freezing model.5.cv3.0.cv3.conv.weight\n","freezing model.5.cv3.0.cv3.bn.weight\n","freezing model.5.cv3.0.cv3.bn.bias\n","freezing model.5.cv3.0.m.0.cv1.conv1.conv.weight\n","freezing model.5.cv3.0.m.0.cv1.conv1.bn.weight\n","freezing model.5.cv3.0.m.0.cv1.conv1.bn.bias\n","freezing model.5.cv3.0.m.0.cv1.conv2.conv.weight\n","freezing model.5.cv3.0.m.0.cv1.conv2.bn.weight\n","freezing model.5.cv3.0.m.0.cv1.conv2.bn.bias\n","freezing model.5.cv3.0.m.0.cv2.conv.weight\n","freezing model.5.cv3.0.m.0.cv2.bn.weight\n","freezing model.5.cv3.0.m.0.cv2.bn.bias\n","freezing model.5.cv3.1.conv.weight\n","freezing model.5.cv3.1.bn.weight\n","freezing model.5.cv3.1.bn.bias\n","freezing model.5.cv4.conv.weight\n","freezing model.5.cv4.bn.weight\n","freezing model.5.cv4.bn.bias\n","freezing model.6.cv1.conv.weight\n","freezing model.6.cv1.bn.weight\n","freezing model.6.cv1.bn.bias\n","freezing model.6.cv2.conv.weight\n","freezing model.6.cv2.bn.weight\n","freezing model.6.cv2.bn.bias\n","freezing model.7.cv1.conv.weight\n","freezing model.7.cv1.bn.weight\n","freezing model.7.cv1.bn.bias\n","freezing model.7.cv2.0.cv1.conv.weight\n","freezing model.7.cv2.0.cv1.bn.weight\n","freezing model.7.cv2.0.cv1.bn.bias\n","freezing model.7.cv2.0.cv2.conv.weight\n","freezing model.7.cv2.0.cv2.bn.weight\n","freezing model.7.cv2.0.cv2.bn.bias\n","freezing model.7.cv2.0.cv3.conv.weight\n","freezing model.7.cv2.0.cv3.bn.weight\n","freezing model.7.cv2.0.cv3.bn.bias\n","freezing model.7.cv2.0.m.0.cv1.conv1.conv.weight\n","freezing model.7.cv2.0.m.0.cv1.conv1.bn.weight\n","freezing model.7.cv2.0.m.0.cv1.conv1.bn.bias\n","freezing model.7.cv2.0.m.0.cv1.conv2.conv.weight\n","freezing model.7.cv2.0.m.0.cv1.conv2.bn.weight\n","freezing model.7.cv2.0.m.0.cv1.conv2.bn.bias\n","freezing model.7.cv2.0.m.0.cv2.conv.weight\n","freezing model.7.cv2.0.m.0.cv2.bn.weight\n","freezing model.7.cv2.0.m.0.cv2.bn.bias\n","freezing model.7.cv2.1.conv.weight\n","freezing model.7.cv2.1.bn.weight\n","freezing model.7.cv2.1.bn.bias\n","freezing model.7.cv3.0.cv1.conv.weight\n","freezing model.7.cv3.0.cv1.bn.weight\n","freezing model.7.cv3.0.cv1.bn.bias\n","freezing model.7.cv3.0.cv2.conv.weight\n","freezing model.7.cv3.0.cv2.bn.weight\n","freezing model.7.cv3.0.cv2.bn.bias\n","freezing model.7.cv3.0.cv3.conv.weight\n","freezing model.7.cv3.0.cv3.bn.weight\n","freezing model.7.cv3.0.cv3.bn.bias\n","freezing model.7.cv3.0.m.0.cv1.conv1.conv.weight\n","freezing model.7.cv3.0.m.0.cv1.conv1.bn.weight\n","freezing model.7.cv3.0.m.0.cv1.conv1.bn.bias\n","freezing model.7.cv3.0.m.0.cv1.conv2.conv.weight\n","freezing model.7.cv3.0.m.0.cv1.conv2.bn.weight\n","freezing model.7.cv3.0.m.0.cv1.conv2.bn.bias\n","freezing model.7.cv3.0.m.0.cv2.conv.weight\n","freezing model.7.cv3.0.m.0.cv2.bn.weight\n","freezing model.7.cv3.0.m.0.cv2.bn.bias\n","freezing model.7.cv3.1.conv.weight\n","freezing model.7.cv3.1.bn.weight\n","freezing model.7.cv3.1.bn.bias\n","freezing model.7.cv4.conv.weight\n","freezing model.7.cv4.bn.weight\n","freezing model.7.cv4.bn.bias\n","freezing model.8.cv1.conv.weight\n","freezing model.8.cv1.bn.weight\n","freezing model.8.cv1.bn.bias\n","freezing model.8.cv2.conv.weight\n","freezing model.8.cv2.bn.weight\n","freezing model.8.cv2.bn.bias\n","freezing model.9.cv1.conv.weight\n","freezing model.9.cv1.bn.weight\n","freezing model.9.cv1.bn.bias\n","freezing model.9.cv2.0.cv1.conv.weight\n","freezing model.9.cv2.0.cv1.bn.weight\n","freezing model.9.cv2.0.cv1.bn.bias\n","freezing model.9.cv2.0.cv2.conv.weight\n","freezing model.9.cv2.0.cv2.bn.weight\n","freezing model.9.cv2.0.cv2.bn.bias\n","freezing model.9.cv2.0.cv3.conv.weight\n","freezing model.9.cv2.0.cv3.bn.weight\n","freezing model.9.cv2.0.cv3.bn.bias\n","freezing model.9.cv2.0.m.0.cv1.conv1.conv.weight\n","freezing model.9.cv2.0.m.0.cv1.conv1.bn.weight\n","freezing model.9.cv2.0.m.0.cv1.conv1.bn.bias\n","freezing model.9.cv2.0.m.0.cv1.conv2.conv.weight\n","freezing model.9.cv2.0.m.0.cv1.conv2.bn.weight\n","freezing model.9.cv2.0.m.0.cv1.conv2.bn.bias\n","freezing model.9.cv2.0.m.0.cv2.conv.weight\n","freezing model.9.cv2.0.m.0.cv2.bn.weight\n","freezing model.9.cv2.0.m.0.cv2.bn.bias\n","freezing model.9.cv2.1.conv.weight\n","freezing model.9.cv2.1.bn.weight\n","freezing model.9.cv2.1.bn.bias\n","freezing model.9.cv3.0.cv1.conv.weight\n","freezing model.9.cv3.0.cv1.bn.weight\n","freezing model.9.cv3.0.cv1.bn.bias\n","freezing model.9.cv3.0.cv2.conv.weight\n","freezing model.9.cv3.0.cv2.bn.weight\n","freezing model.9.cv3.0.cv2.bn.bias\n","freezing model.9.cv3.0.cv3.conv.weight\n","freezing model.9.cv3.0.cv3.bn.weight\n","freezing model.9.cv3.0.cv3.bn.bias\n","freezing model.9.cv3.0.m.0.cv1.conv1.conv.weight\n","freezing model.9.cv3.0.m.0.cv1.conv1.bn.weight\n","freezing model.9.cv3.0.m.0.cv1.conv1.bn.bias\n","freezing model.9.cv3.0.m.0.cv1.conv2.conv.weight\n","freezing model.9.cv3.0.m.0.cv1.conv2.bn.weight\n","freezing model.9.cv3.0.m.0.cv1.conv2.bn.bias\n","freezing model.9.cv3.0.m.0.cv2.conv.weight\n","freezing model.9.cv3.0.m.0.cv2.bn.weight\n","freezing model.9.cv3.0.m.0.cv2.bn.bias\n","freezing model.9.cv3.1.conv.weight\n","freezing model.9.cv3.1.bn.weight\n","freezing model.9.cv3.1.bn.bias\n","freezing model.9.cv4.conv.weight\n","freezing model.9.cv4.bn.weight\n","freezing model.9.cv4.bn.bias\n","\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01) with parameter groups 238 weight(decay=0.0), 255 weight(decay=0.0005), 253 bias\n","\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels...:   0%|          | 0/948 00:00\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 1 images, 0 backgrounds, 0 corrupt:   0%|          | 1/948 00:05\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 70 images, 0 backgrounds, 0 corrupt:   7%|▋         | 70/948 00:05\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 157 images, 0 backgrounds, 0 corrupt:  17%|█▋        | 157/948 00:05\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 245 images, 0 backgrounds, 0 corrupt:  26%|██▌       | 245/948 00:05\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 341 images, 0 backgrounds, 0 corrupt:  36%|███▌      | 341/948 00:05\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 425 images, 0 backgrounds, 0 corrupt:  45%|████▍     | 425/948 00:05\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 520 images, 0 backgrounds, 0 corrupt:  55%|█████▍    | 520/948 00:05\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 607 images, 0 backgrounds, 0 corrupt:  64%|██████▍   | 607/948 00:06\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 694 images, 0 backgrounds, 0 corrupt:  73%|███████▎  | 694/948 00:06\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 783 images, 0 backgrounds, 0 corrupt:  83%|████████▎ | 783/948 00:06\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 872 images, 0 backgrounds, 0 corrupt:  92%|█████████▏| 872/948 00:06\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels... 948 images, 0 backgrounds, 0 corrupt: 100%|██████████| 948/948 00:06\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\train\\labels.cache\n","\n","\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\valid\\labels...:   0%|          | 0/66 00:00\n","\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\valid\\labels... 1 images, 0 backgrounds, 0 corrupt:   2%|▏         | 1/66 00:06\n","\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\valid\\labels... 24 images, 0 backgrounds, 0 corrupt:  36%|███▋      | 24/66 00:06\n","\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\valid\\labels... 64 images, 0 backgrounds, 0 corrupt:  97%|█████████▋| 64/66 00:06\n","\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\valid\\labels... 66 images, 0 backgrounds, 0 corrupt: 100%|██████████| 66/66 00:06\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\datasets\\set1\\valid\\labels.cache\n","Plotting labels to runs\\train\\yolov9-c_uav_size480_ep20_batch16_dataset-set1_optim-Adam_2024-05-08_20-54-20\\labels.jpg... \n","Image sizes 480 train, 480 val\n","Using 12 dataloader workers\n","Logging results to \u001b[1mruns\\train\\yolov9-c_uav_size480_ep20_batch16_dataset-set1_optim-Adam_2024-05-08_20-54-20\u001b[0m\n","Starting training for 20 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\n","  0%|          | 0/60 00:00\n","       0/19      6.86G      2.699      4.018      2.708         31        480:   0%|          | 0/60 00:03WARNING  TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n","\n","       0/19      6.86G      2.699      4.018      2.708         31        480:   2%|▏         | 1/60 00:06\n","       0/19      7.11G      2.629      3.872      2.628         44        480:   2%|▏         | 1/60 00:08\n","       0/19      7.11G      2.629      3.872      2.628         44        480:   3%|▎         | 2/60 00:08\n","       0/19      7.11G      2.678      3.955       2.56         27        480:   3%|▎         | 2/60 00:10\n","       0/19      7.11G      2.678      3.955       2.56         27        480:   5%|▌         | 3/60 00:10\n","       0/19      7.11G      2.677      3.891      2.554         30        480:   5%|▌         | 3/60 00:11\n","       0/19      7.11G      2.677      3.891      2.554         30        480:   7%|▋         | 4/60 00:11\n","       0/19      7.11G       2.72      3.863      2.566         42        480:   7%|▋         | 4/60 00:12\n","       0/19      7.11G       2.72      3.863      2.566         42        480:   8%|▊         | 5/60 00:12\n","       0/19      7.11G      2.641      3.831      2.524         44        480:   8%|▊         | 5/60 00:14\n","       0/19      7.11G      2.641      3.831      2.524         44        480:  10%|█         | 6/60 00:14\n","       0/19       7.4G      2.633      11.93      2.497         45        480:  10%|█         | 6/60 00:16\n","       0/19       7.4G      2.633      11.93      2.497         45        480:  12%|█▏        | 7/60 00:16\n","       0/19       7.4G      2.659      19.38       2.49         40        480:  12%|█▏        | 7/60 00:18\n","       0/19       7.4G      2.659      19.38       2.49         40        480:  13%|█▎        | 8/60 00:18\n","       0/19       7.4G      2.643      24.78       2.49         40        480:  13%|█▎        | 8/60 00:20\n","       0/19       7.4G      2.643      24.78       2.49         40        480:  15%|█▌        | 9/60 00:20\n","       0/19       7.4G      2.635      29.18      2.493         37        480:  15%|█▌        | 9/60 00:22\n","       0/19       7.4G      2.635      29.18      2.493         37        480:  17%|█▋        | 10/60 00:22\n","       0/19       7.4G      2.655      33.38      2.494         45        480:  17%|█▋        | 10/60 00:23\n","       0/19       7.4G      2.655      33.38      2.494         45        480:  18%|█▊        | 11/60 00:23\n","       0/19       7.4G      2.651      36.44      2.504         40        480:  18%|█▊        | 11/60 00:25\n","       0/19       7.4G      2.651      36.44      2.504         40        480:  20%|██        | 12/60 00:25\n","       0/19       7.4G      2.759      46.92      2.573         33        480:  20%|██        | 12/60 00:30\n","       0/19       7.4G      2.759      46.92      2.573         33        480:  22%|██▏       | 13/60 00:30\n","       0/19       7.4G      2.842       52.8       2.63         36        480:  22%|██▏       | 13/60 00:31\n","       0/19       7.4G      2.842       52.8       2.63         36        480:  23%|██▎       | 14/60 00:31\n","       0/19       7.4G      2.951      62.92      2.707         23        480:  23%|██▎       | 14/60 00:33\n","       0/19       7.4G      2.951      62.92      2.707         23        480:  25%|██▌       | 15/60 00:33\n","       0/19       7.4G      3.022      66.24      2.755         44        480:  25%|██▌       | 15/60 00:34\n","       0/19       7.4G      3.022      66.24      2.755         44        480:  27%|██▋       | 16/60 00:34\n","       0/19       7.4G      3.096      65.56      2.826         31        480:  27%|██▋       | 16/60 00:36\n","       0/19       7.4G      3.096      65.56      2.826         31        480:  28%|██▊       | 17/60 00:36\n","       0/19       7.4G      3.175      62.51      2.868         60        480:  28%|██▊       | 17/60 00:38\n","       0/19       7.4G      3.175      62.51      2.868         60        480:  30%|███       | 18/60 00:38\n","       0/19       7.4G      3.206      59.48      2.902         40        480:  30%|███       | 18/60 00:40\n","       0/19       7.4G      3.206      59.48      2.902         40        480:  32%|███▏      | 19/60 00:40\n","       0/19       7.4G      3.264      56.72      2.977         46        480:  32%|███▏      | 19/60 00:43\n","       0/19       7.4G      3.264      56.72      2.977         46        480:  33%|███▎      | 20/60 00:43\n","       0/19       7.4G      3.307      54.22       3.01         43        480:  33%|███▎      | 20/60 00:45\n","       0/19       7.4G      3.307      54.22       3.01         43        480:  35%|███▌      | 21/60 00:45\n","       0/19       7.4G      3.354      51.93      3.043         47        480:  35%|███▌      | 21/60 00:46\n","       0/19       7.4G      3.354      51.93      3.043         47        480:  37%|███▋      | 22/60 00:46\n","       0/19       7.4G      3.373      49.83      3.057         37        480:  37%|███▋      | 22/60 00:48\n","       0/19       7.4G      3.373      49.83      3.057         37        480:  38%|███▊      | 23/60 00:48\n","       0/19       7.4G      3.408      47.91      3.083         37        480:  38%|███▊      | 23/60 00:50\n","       0/19       7.4G      3.408      47.91      3.083         37        480:  40%|████      | 24/60 00:50\n","       0/19       7.4G      3.422      46.15        3.1         39        480:  40%|████      | 24/60 00:51\n","       0/19       7.4G      3.422      46.15        3.1         39        480:  42%|████▏     | 25/60 00:51\n","       0/19       7.4G      3.446      44.54      3.114         37        480:  42%|████▏     | 25/60 00:53\n","       0/19       7.4G      3.446      44.54      3.114         37        480:  43%|████▎     | 26/60 00:53\n","       0/19       7.4G      3.456      43.04       3.13         33        480:  43%|████▎     | 26/60 00:55\n","       0/19       7.4G      3.456      43.04       3.13         33        480:  45%|████▌     | 27/60 00:55\n","       0/19       7.4G      3.475      41.62      3.144         43        480:  45%|████▌     | 27/60 00:57\n","       0/19       7.4G      3.475      41.62      3.144         43        480:  47%|████▋     | 28/60 00:57\n","       0/19       7.4G      3.489       40.3      3.158         36        480:  47%|████▋     | 28/60 01:00\n","       0/19       7.4G      3.489       40.3      3.158         36        480:  48%|████▊     | 29/60 01:00\n","       0/19       7.4G      3.494      39.07      3.165         44        480:  48%|████▊     | 29/60 01:02\n","       0/19       7.4G      3.494      39.07      3.165         44        480:  50%|█████     | 30/60 01:02\n","       0/19       7.4G      3.498      37.91      3.167         39        480:  50%|█████     | 30/60 01:04\n","       0/19       7.4G      3.498      37.91      3.167         39        480:  52%|█████▏    | 31/60 01:04\n","       0/19       7.4G      3.517      36.83      3.177         37        480:  52%|█████▏    | 31/60 01:05\n","       0/19       7.4G      3.517      36.83      3.177         37        480:  53%|█████▎    | 32/60 01:05\n","       0/19       7.4G      3.527       35.8      3.187         39        480:  53%|█████▎    | 32/60 01:07\n","       0/19       7.4G      3.527       35.8      3.187         39        480:  55%|█████▌    | 33/60 01:07\n","       0/19       7.4G       3.53      34.84      3.196         33        480:  55%|█████▌    | 33/60 01:09\n","       0/19       7.4G       3.53      34.84      3.196         33        480:  57%|█████▋    | 34/60 01:09\n","       0/19       7.4G       3.53      34.84      3.196         33        480:  57%|█████▋    | 34/60 01:09\n","Traceback (most recent call last):\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\train_dual.py\", line 644, in <module>\n","    main(opt)\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\train_dual.py\", line 538, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\train_dual.py\", line 288, in train\n","    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\tqdm\\std.py\", line 1195, in __iter__\n","    for obj in iterable:\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\utils\\dataloaders.py\", line 170, in __iter__\n","    yield next(self.iterator)\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 628, in __next__\n","    data = self._next_data()\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1333, in _next_data\n","    return self._process_data(data)\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1359, in _process_data\n","    data.reraise()\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\_utils.py\", line 543, in reraise\n","    raise exception\n","cv2.error: Caught error in DataLoader worker process 10.\n","Original Traceback (most recent call last):\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n","    data = fetcher.fetch(index)\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"d:\\PROG\\Anaconda\\envs\\py310vision\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\utils\\dataloaders.py\", line 656, in __getitem__\n","    img, labels = self.load_mosaic(index)\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\utils\\dataloaders.py\", line 755, in load_mosaic\n","    img, _, (h, w) = self.load_image(index)\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\utils\\dataloaders.py\", line 730, in load_image\n","    im = cv2.imread(f)  # BGR\n","  File \"D:\\OKUL\\YL\\Dersler\\BLG_561E_Deep_Learning\\term_project\\UAV_Detection\\yolov9\\utils\\general.py\", line 1118, in imread\n","    return cv2.imdecode(np.fromfile(path, np.uint8), flags)\n","cv2.error: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1228800 bytes in function 'cv::OutOfMemoryError'\n","\n","\n"]}],"source":["print(\"init\")\n","\n","%cd yolov9\n","\n","\n","# import os\n","# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized\n","\n","!wandb disabled\n","\n","print(\"begin train\")\n","\n","!python train_dual.py \\\n"," --name {RUN_NAME} \\\n"," --epochs {EPOCHS} \\\n"," --workers {WORKERS} \\\n"," --device 0 \\\n"," --batch-size {BATCH_SIZE} \\\n"," --weights weights/{PRETRAINED} \\\n"," --img {IMG_SIZE} \\\n"," --freeze {FREEZE} \\\n"," --patience {PATIENCE} \\\n"," --optimizer {OPTIMIZER} \\\n"," --cfg models/detect/{MODEL_CFG_FILE}.yaml \\\n"," --data data/{DATA_CFG_FILE}.yaml \\\n"," --hyp data/hyps/{HYP_FILE}.yaml \n"]},{"cell_type":"markdown","metadata":{},"source":["### Google Drive - Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":714},"executionInfo":{"elapsed":135122,"status":"ok","timestamp":1670749224728,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"zfYMTCzP-mLM","outputId":"8c4afb4c-41ff-4908-84dc-02f0fe359233"},"outputs":[],"source":["# #-- zip the output run folder and download it.\n","# #-- zip -r {out_name} {path_to_folder_to_zip}\n","# !zip -r /content/{RUN_NAME}.zip /content/yolov9/runs/train/{RUN_NAME}\n","# from google.colab import files\n","# files.download(f\"/content/{RUN_NAME}.zip\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":8291,"status":"ok","timestamp":1670749321771,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"AfiHlc0Tqgcp","outputId":"1958103f-9738-4ff3-9b9b-9514bced8aa7"},"outputs":[],"source":["# import shutil\n","# shutil.copy2(f\"/content/{RUN_NAME}.zip\", \n","#              f\"/content/drive/MyDrive/BLG_561E_DL/term_project/\")"]},{"cell_type":"markdown","metadata":{},"source":["### Memory Check"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":888,"status":"ok","timestamp":1670178193222,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"LGj_fZD7YKWE","outputId":"29ddc80c-0a9b-4cb5-e9e2-728c37f36a43"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11498,"status":"ok","timestamp":1670178270883,"user":{"displayName":"Alperen GENÇOĞLU","userId":"15538304633100582897"},"user_tz":-180},"id":"j_nlmblBYbSW","outputId":"8d9193e8-f8c0-4d80-961e-fb0f215a6f72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gen RAM Free: 16.7 GB  |     Proc size: 92.0 MB\n","GPU RAM Free: 6822MB | Used: 1215MB | Util  15% | Total     8192MB\n"]}],"source":["# memory footprint support libraries/code\n","# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","# !pip install gputil\n","# !pip install psutil\n","# !pip install humanize\n","\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printmem():\n","    process = psutil.Process(os.getpid())\n","    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n","    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printmem()\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN4j1cKQwbD78pXNHvroYLL","mount_file_id":"1IFwRUVdlRJl6c4iF9oSgzkQuPB-Z_1Uh","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"py310vision","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"e20c3a6b5aa3ae2a151390609f6472b95650c981f6a561cec086c1d4a454e867"}}},"nbformat":4,"nbformat_minor":0}
